{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4678137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install splinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9327d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b759c817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully selected 'all' option from the dropdown.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()  # Ensure you have ChromeDriver installed\n",
    "driver.get(\"https://ircc.canada.ca/english/newcomers/services/index.asp#table1\")\n",
    "\n",
    "# Wait for the dropdown to be visible and select \"all\"\n",
    "wait = WebDriverWait(driver, 20)\n",
    "try:\n",
    "    dropdown = wait.until(EC.visibility_of_element_located((By.NAME, \"table1_length\")))\n",
    "    dropdown.click()\n",
    "    all_option = driver.find_element(By.XPATH, \"//option[@value='-1']\")\n",
    "    all_option.click()\n",
    "    print(\"Successfully selected 'all' option from the dropdown.\")\n",
    "\n",
    "    # Allow some time for all the rows to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"id\": \"table1\"})\n",
    "\n",
    "    # List to store all extracted data\n",
    "    all_page_data = []\n",
    "\n",
    "    # Find all rows (tr) in the table\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    # Iterate through each row to extract data\n",
    "    for row in rows:\n",
    "        # Extract Organisation Name\n",
    "        org_name_element = row.find(\"a\", string=True)  # Find the <a> tag containing the org name\n",
    "        org_name = org_name_element.text.strip() if org_name_element else None\n",
    "\n",
    "        # Extract Organisation Website\n",
    "        org_website_element = row.find(\"a\", href=True)\n",
    "        org_website = org_website_element[\"href\"] if org_website_element else None\n",
    "        \n",
    "        # Extract all <a> tags with href in the row\n",
    "        org_address_elements = row.find_all(\"a\", href=True)\n",
    "\n",
    "        # Extract the Google Maps link\n",
    "        org_googlemap_element = org_address_elements[1] if len(org_address_elements) > 1 else None\n",
    "        org_googlemap = org_googlemap_element[\"href\"] if org_googlemap_element else None\n",
    "\n",
    "        # Extract Organisation Address\n",
    "        org_address_element = org_address_elements[1] if len(org_address_elements) > 1 else None  # Get the second <a> tag\n",
    "        org_address = org_address_element.text.strip() if org_address_element else None\n",
    "\n",
    "        # Extract Languages of Service\n",
    "        lang_element = row.find(\"td\")  # Assume languages are in a <td> tag\n",
    "        if lang_element and \"Languages of service:\" in lang_element.text:\n",
    "            org_languages = lang_element.text.split(\"Languages of service:\")[1].strip()\n",
    "        else:\n",
    "            org_languages = None\n",
    "\n",
    "        # Extract Services\n",
    "        services_elements = row.find_all(\"li\", class_=\"nowrap\")\n",
    "        org_services = [service.text.strip() for service in services_elements]\n",
    "        \n",
    "        # Extract details from all <td> elements\n",
    "        all_td_text = []\n",
    "        td_elements = row.find_all(\"td\")\n",
    "        for td in td_elements:\n",
    "            for a_tag in td.find_all(\"a\"):\n",
    "                a_tag.decompose()  # Remove <a> tags to avoid hyperlink text\n",
    "            cell_text = td.get_text(strip=True)\n",
    "            if cell_text:\n",
    "                all_td_text.append(cell_text)\n",
    "        org_details = \"/\".join(all_td_text)\n",
    "\n",
    "        # Organize extracted data into a dictionary\n",
    "        item_data = {\n",
    "            \"Organisation Name\": org_name,\n",
    "            \"Organisation Website\": org_website,\n",
    "            \"Organisation Address\": org_address,\n",
    "            \"Organisation Languages\": org_languages,\n",
    "            \"Organisation Services\": org_services,\n",
    "            \"Organisation Google Map\": org_googlemap,\n",
    "            \"Organisation Details\": org_details,\n",
    "        }\n",
    "\n",
    "        # Append the data to the list\n",
    "        all_page_data.append(item_data)\n",
    "\n",
    "    # Convert the collected data into a DataFrame\n",
    "    df = pd.DataFrame(all_page_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error encountered: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the driver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d7e038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"scraped_organisations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79be568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Splinter Browser\n",
    "# with Browser('chrome') as browser:\n",
    "#     # Define base URL\n",
    "#     url = 'https://ircc.canada.ca/english/newcomers/services/index.asp'\n",
    "#     browser.visit(url)\n",
    "#     html = browser.html\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#     # List to store all extracted data\n",
    "#     all_page_data = []\n",
    "\n",
    "#     # Find all rows (tr) in the table\n",
    "#     rows = soup.find_all('tr')\n",
    "\n",
    "#     # Iterate through each row to extract data\n",
    "#     for row in rows:\n",
    "#         # Extract Organisation Name\n",
    "#         org_name_element = row.find('a', string=True)  # Find the <a> tag containing the org name\n",
    "#         org_name = org_name_element.text.strip() if org_name_element else None\n",
    "\n",
    "#         # Extract Organisation Website\n",
    "#         org_website_element = row.find('a', href=True)\n",
    "#         org_website = org_website_element['href'] if org_website_element else None\n",
    "        \n",
    "#         # Extract all <a> tags with href in the row\n",
    "#         org_address_elements = row.find_all('a', href=True)\n",
    "\n",
    "#         # Extract the second <a> tag, assuming it's the Google Maps link\n",
    "#         org_googlemap_element = org_address_elements[1] if len(org_address_elements) > 1 else None\n",
    "#         org_googlemap = org_googlemap_element['href'] if org_googlemap_element else None\n",
    "\n",
    "#         # Extract the second <a> tag in the row (if it exists)\n",
    "#         org_address_element = org_address_elements[1] if len(org_address_elements) > 1 else None  # Get the second <a> tag\n",
    "#         org_address = org_address_element.text.strip() if org_address_element else None\n",
    "\n",
    "#         # Extract Languages of Service\n",
    "#         lang_element = row.find('td')  # Assume languages are in a <td> tag\n",
    "#         if lang_element and \"Languages of service:\" in lang_element.text:\n",
    "#             org_languages = lang_element.text.split(\"Languages of service:\")[1].strip()\n",
    "#         else:\n",
    "#             org_languages = None\n",
    "\n",
    "#         # Extract Services\n",
    "#         services_elements = row.find_all('li', class_=\"nowrap\")\n",
    "#         org_services = [service.text.strip() for service in services_elements]\n",
    "        \n",
    "#         # Initialize an empty list to store the texts of all <td> elements\n",
    "#         all_td_text = []\n",
    "\n",
    "#         # Find all <td> tags in the row (table data cells)\n",
    "#         td_elements = row.find_all('td')\n",
    "        \n",
    "#         # Iterate through each <td> tag in the row\n",
    "#         for td in td_elements:\n",
    "#             # Remove all <a> tags to avoid extracting the hyperlink text\n",
    "#             for a_tag in td.find_all('a'):\n",
    "#                 a_tag.decompose()  # This will remove the <a> tag and its content from the <td>\n",
    "            \n",
    "#             # Get the text inside the <td> and strip any surrounding whitespace\n",
    "#             cell_text = td.get_text(strip=True)\n",
    "            \n",
    "#             if cell_text:  # Add non-empty text to the list\n",
    "#                 all_td_text.append(cell_text)\n",
    "\n",
    "#         # Join the list of texts from all <td> elements into a single string\n",
    "#         org_details = '/'.join(all_td_text)\n",
    "\n",
    "#         # Organize extracted data into a dictionary\n",
    "#         item_data = {\n",
    "#             'Organisation Name': org_name,\n",
    "#             'Organisation Website': org_website,\n",
    "#             'Organisation Address': org_address,\n",
    "#             'Organisation Languages': org_languages,\n",
    "#             'Organisation Services': org_services,\n",
    "#             'Organisation Google Map': org_googlemap,\n",
    "#             'Organisation Details': org_details  # Store the combined <td> contents\n",
    "#         }\n",
    "\n",
    "#         # Append the data to the list\n",
    "#         all_page_data.append(item_data)\n",
    "\n",
    "#     # Print or process the collected data\n",
    "#     for data in all_page_data:\n",
    "#         print(data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
